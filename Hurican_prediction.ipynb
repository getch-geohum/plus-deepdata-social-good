{"cells":[{"cell_type":"markdown","metadata":{"id":"EZrPtfccl1ze"},"source":["\n","<br> Theme of the material:**Time series analysis for hurricane prediction using LSTMs** <br>\n","Prepared by: *Getachew Workineh Gella* <br>\n","Paris Lodron University of Salzburg (PLUS), Department of Applied Geoinformatics<br>\n","email: getachewworkineh.gella@plus.ac.at"]},{"cell_type":"markdown","metadata":{"id":"DfIvh_-Zl1zg"},"source":["**Note!:** *Some commented cells are optional based on availability of data and implemntation strategy*\n","#### Loading Necessary packages for data reading, exploration and sequence modeling"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CW7JDkUhmANe","executionInfo":{"status":"ok","timestamp":1733158204941,"user_tz":-180,"elapsed":35887,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}},"outputId":"4a2cda49-df20-437d-de44-3db3130a9740"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","execution_count":2,"metadata":{"id":"P0kGKW5ql1zi","executionInfo":{"status":"ok","timestamp":1733158220384,"user_tz":-180,"elapsed":10068,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","import pandas as pd\n","import numpy as np\n","import os\n","import tensorflow as tf\n","import seaborn as sbn\n","import matplotlib.pyplot as plt\n","import datetime\n","import random\n","from sklearn import preprocessing\n","from sklearn.model_selection import train_test_split  # if sample split is not done by user defined code\n","from tensorflow.keras.layers import Input, LSTM, Dropout, Bidirectional, BatchNormalization, Dense\n","from tensorflow.keras.models import Sequential, Model\n","from tensorflow.keras.optimizers import SGD, Adam, RMSprop"]},{"cell_type":"markdown","metadata":{"id":"enZ-vP-wl1zj"},"source":["#### Read the data\n","It should be noted that while reading the data using pandas dataframe approach, it is possible to manage nodata values on the flight or remove it after reading the data\n","For this excercise I have opted to tell the datafram reader to consider -999 as No data value"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"1UciLEIBl1zk","executionInfo":{"status":"ok","timestamp":1733158320591,"user_tz":-180,"elapsed":686,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["path = '/content/drive/MyDrive/plus-deepdata-social-good-master/plus-deepdata-social-good-master/atlantic.csv'\n","PD = pd.read_csv(path, na_values=-999)   # -999"]},{"cell_type":"markdown","metadata":{"id":"Mq65AiRrl1zl"},"source":["#### Inspect the attribute names(column names) and respective data types.\n","This will enable enable to select proper data conversion options in further processing steps.\n","It shoud be noted that in pandas dataframe string objects are considered as object data types"]},{"cell_type":"code","execution_count":40,"metadata":{"id":"GLBNWxvdl1zm","executionInfo":{"status":"ok","timestamp":1733160779305,"user_tz":-180,"elapsed":389,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["print('The number of attributes in the dataset is : {}'.format(len(list(PD.keys()))))\n","col_names = list(PD.keys())\n","print('Attribute -> data type')\n","for name in col_names:\n","    print('{} -> {}'.format(name, PD[name].dtype))"]},{"cell_type":"markdown","metadata":{"id":"qzG6oDsrl1zm"},"source":["### Inspect the first five and the last five rows with full columns.\n","<div style=\"text-align: justify\"> This gives overll picture of data. As can be seen from the table displayed below, the first few rows\n","are dominated by incomplete observations except for latitude, longitue of the herican cenetrs. As can be seen from Herican Unique ID and Unique name count, names are less than Unique IDS. This is mainly\n","because of repeatedly happening herricaens happening at some geographies were given the same names but different ids and some hericanes were given a name of UnKnown.Therefore, as every trajectory monitoring is based on tagged unique herican name, in the feature engineering/preparation section, unique IDS will use to organize the data.   </div>\n"]},{"cell_type":"code","execution_count":5,"metadata":{"scrolled":true,"id":"fITgloxGl1zn","executionInfo":{"status":"ok","timestamp":1733161524384,"user_tz":-180,"elapsed":353,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["PD.head()"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"G49v1Lo5l1zo","executionInfo":{"status":"ok","timestamp":1733161510483,"user_tz":-180,"elapsed":380,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["# inspect the data structure of the last five rows with all columns\n","PD.tail()"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"JsRhj7uHl1zp","executionInfo":{"status":"ok","timestamp":1733160805421,"user_tz":-180,"elapsed":344,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["# count names and IDS of the hericanes\n","ids = np.unique(PD['ID'])\n","print('Number of total hurican unique IDS: {}'.format(len(ids)))\n","names = np.unique(PD['Name'])\n","print('Number of total hurican unique names: {}'.format(len(names)))"]},{"cell_type":"markdown","metadata":{"id":"4oO23nMtl1zq"},"source":["# Data conversion and cleaning\n","<div style=\"text-align: justify\"> As can be seen from the tables displayed the following observations were taken into consideration <br>\n","1. Observations with massive incomplete data(have to be excluded from analysis) <br>\n","    2. Some rows whith some missing values were imputed with mean values of observations from the same herican. It should ne noted that, this should betaken with great caution as it will introduce extra noise to the data <br>\n","    3. Latitude and longitude values which are the main interest of this analysis were given as textual information whith hemispheric information(North/south, west/east).Using this information, the coordinates were converted to float longitude and latutidude. X coordinate in west and Y coordinate in south hemispher were converted to negative values. this gives a universal/uniform representation of coordinates <br>\n","    4. Date value is provided as string literal (YYMMDD) and then converted to Day of the Year (DOY) format <br>\n","    5. The Time value is converted from thousand notation to normal 24 hours scale which is more meaningfull </div>\n","\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"d1NBLq-6l1zq","executionInfo":{"status":"ok","timestamp":1733158402515,"user_tz":-180,"elapsed":608,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["def manage_long(df):\n","    '''\n","    Converts text longitude into numeric longitude\n","    the west is converted into negative values while the east is converted to positive values\n","    df: a data frame containing longitude value\n","    returns Longitude value with numeric representation without hemispheric information\n","    '''\n","    dff = df.copy()\n","    long = np.array(dff['Longitude'])\n","\n","    for i in range(long.shape[0]):\n","        if \"W\" in long[i]:\n","            long[i] = float(long[i].replace('W',''))*-1\n","        elif 'E' in long[i]:\n","            long[i] = float(long[i].replace('E',''))*1\n","        else:\n","            raise ValueError('The value has no direction indicator')\n","    return long"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"Z1pnra18l1zq","executionInfo":{"status":"ok","timestamp":1733158448697,"user_tz":-180,"elapsed":405,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["def manage_lat(df):\n","    '''\n","    Converts text latitude into numeric latitude\n","    the South is converted into negative values while the North is converted to positive values\n","    df: a data frame containing Latitude value\n","    returns Latitude value with numeric representation without hemispheric information\n","    '''\n","    dff = df.copy()\n","    lat = np.array(dff['Latitude'])\n","\n","    for i in range(lat.shape[0]):\n","        if \"S\" in lat[i]:\n","            lat[i] = float(lat[i].replace('S',''))*-1\n","        elif \"N\" in lat[i]:\n","            lat[i] = float(lat[i].replace('N',''))*1\n","        else:\n","            raise ValueError('The value has no direction indicator')\n","    return lat"]},{"cell_type":"markdown","metadata":{"id":"wIP0igpFl1zr"},"source":["##  Remove a subset of the dataset which have not sufficient observations and check presence of nodata values\n","As can be seen from the cell output below, even after droping a subset of the dataset which have nodata value there are still 80 records with nodata value for some columns/attributes."]},{"cell_type":"code","execution_count":49,"metadata":{"id":"sADOVDnTl1zr","executionInfo":{"status":"ok","timestamp":1733160888086,"user_tz":-180,"elapsed":822,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["PD_n = PD.loc[43106:]\n","PD_n.isna().sum()"]},{"cell_type":"markdown","metadata":{"id":"OMoBKa7gl1zt"},"source":["## Convert latitude and longitude from hemispheric representation to actual numeric values\n","As noted above, if its West and South values were become negative coordinate values. Chceck the converted longitude and latutude values by ploting in a 2D cartesian space"]},{"cell_type":"code","execution_count":51,"metadata":{"id":"AeZ6ZBb-l1zt","executionInfo":{"status":"ok","timestamp":1733160904151,"user_tz":-180,"elapsed":932,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["PD_n['Latitude'] = manage_lat(PD_n)\n","PD_n['Longitude'] = manage_long(PD_n)\n","PD_n[[\"Latitude\", \"Longitude\"]] = PD_n[[\"Latitude\", \"Longitude\"]].apply(pd.to_numeric)\n","latlong = PD_n[[\"Latitude\", \"Longitude\"]]\n","latlong.head()"]},{"cell_type":"markdown","source":["## Visalize the data to understand spatiasense"],"metadata":{"id":"P4oexuXvr8cR"}},{"cell_type":"code","execution_count":53,"metadata":{"id":"T05fviDFl1zu","executionInfo":{"status":"ok","timestamp":1733160919546,"user_tz":-180,"elapsed":653,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["PD_n.head()\n","names = np.unique(PD_n['Name'])\n","for name in names:\n","    dfd = PD_n[PD_n['Name'] == name]\n","    plt.plot(dfd['Longitude'], dfd['Latitude'], label=name)\n","plt.show()"]},{"cell_type":"markdown","source":[],"metadata":{"id":"SemIiKtir61E"}},{"cell_type":"markdown","metadata":{"id":"gV1Hal70l1zu"},"source":[]},{"cell_type":"markdown","metadata":{"id":"f2_CmveNl1zu"},"source":["## Convert time into 24 hour scale and date into Day of The Year (DOY) format, and inspect the converted date"]},{"cell_type":"code","execution_count":55,"metadata":{"id":"66mtf2gfl1zu","executionInfo":{"status":"ok","timestamp":1733160933840,"user_tz":-180,"elapsed":571,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["PD_n['Date'] = [datetime.datetime.strptime(str(PD_n['Date'].iloc[i]),'%Y%m%d').timetuple().tm_yday for i in range(PD_n.shape[0])]\n","PD_n['Time'] = PD_n['Time']/100   # convert time into proper 24 hourse scale\n","datetime = PD_n[[\"Date\", \"Time\"]]\n","datetime.head()"]},{"cell_type":"markdown","metadata":{"id":"R7O-cb_zl1zv"},"source":["## Inspect if there is highly outocorrelating attribute in the dataframe\n","Please note that panda data frame correlation analysis ignores non-numeric columns from correlation computation. This selection of numeric values could also be done mannually. As indicated in the correlation matrix plot, the Minimim pressure and Maximum wind has negatively correlated. As this values are very important for hericane prediction and monitoring I would prefer to keep it."]},{"cell_type":"markdown","source":["Remove nodata value and replace with means. here there is no hard rule how to replace no data vale"],"metadata":{"id":"1EHlr4Ho84zP"}},{"cell_type":"code","source":["# replace Nans with column means per hericane based on herican ID\n","ids = list(np.unique(PD_n['ID']))\n","for i in range(len(ids)):\n","    sub = PD_n[PD_n['ID']==ids[i]]\n","    sub=sub.fillna(sub.drop(columns=['Name', 'Event', 'Status','ID']).mean())\n","    PD_n[PD_n['ID']==ids[i]] = sub\n","PD_n.isna().sum()   # check the na removed dataframe"],"metadata":{"id":"R3evJSjyztFl","executionInfo":{"status":"ok","timestamp":1733160949866,"user_tz":-180,"elapsed":723,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"execution_count":56,"outputs":[]},{"cell_type":"markdown","source":[],"metadata":{"id":"YBUpEoG6ti7A"}},{"cell_type":"markdown","source":["## Plot cross correlation and inspect the spatial and tenporal correlation"],"metadata":{"id":"JPtkQtfktlOF"}},{"cell_type":"code","execution_count":57,"metadata":{"id":"46hyCgWNl1zv","executionInfo":{"status":"ok","timestamp":1733160976586,"user_tz":-180,"elapsed":500,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["crmtx = round(PD_n.drop(columns=['Name', 'Event', 'Status', 'ID']).corr(), 3)\n","plt.figure(figsize=(22,22))\n","sbn.heatmap(crmtx, annot=True)\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"plrjzhAel1zv"},"source":["## Understand data distribution\n","Inspect the histogram/frequency dstribution of each fields/atrributes. As can be seen from the histogram plots, most of the attributes related to wind speed are with left skwed(right tailed) distributions. Latitude Longitude and Time have almost normal distribition."]},{"cell_type":"code","execution_count":60,"metadata":{"id":"zjQc9FkHl1zv","executionInfo":{"status":"ok","timestamp":1733160996381,"user_tz":-180,"elapsed":921,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["PD_n.hist(bins=40, figsize=(22,22))\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"9JvFQ4sfl1zw"},"source":["## Inspect the statistics of each numeric column\n","As can be seen from descriptive statistics there are 5999 rows/data points. Please note that this data points were irrespective of unique hericane ID or name."]},{"cell_type":"code","execution_count":61,"metadata":{"id":"nm_BceAil1zw","executionInfo":{"status":"ok","timestamp":1733161005476,"user_tz":-180,"elapsed":329,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["PD_n.describe().transpose()"]},{"cell_type":"markdown","metadata":{"id":"qlpbmCJPl1zw"},"source":["## Normalization (Optional and not implemented)\n","This is an optional phase. As I am using Latitude and longitude as feature plus label, normalizing this value will affect the semantic meaning/validity of the attribute that, this step is skipped.\n","The implementation is provided in the function \"prepare_samples\" below that if needed, can turn the parameter normalize into True"]},{"cell_type":"markdown","metadata":{"id":"kgoBSYtNl1zx"},"source":["### Convert dataframe to an array wich is suitable for LSTM\n","As the problem is coined as multivariate sequence to sequence prediction, The data is prepared as follows. Time series observation for each unique herican is converted into a 3-dimensional numpy array of [B,T, A] where B is number of batchs, T is sequence length(time window) to model subsequent event which is 4 and A is number of attributes which is 18.\n","\n","In train test split and sampling, the following purposive customization is done. When the data is segregated, the number of rows allocated for trainning were not sufficient that, I simply randomly select testing hericanes. As validation samples are only used to track the learning process, I train without validation samples. This all is to use many data points for model training.\n","\n","But the code used for sampling, generation of trainingand testing samples are commented and can be used when sufficient data is available or if one decided to work on only hericane coordinates."]},{"cell_type":"markdown","metadata":{"id":"FFyHokF-l1zx"},"source":["Deasign:\n","    Features: all observations within previous time t units(24 hours)\n","    Label: latitude and longitue at t+1 units\n","    As indicated in figure below, column A contains all the attributes at time window t while Lat and long are modeeled as labels\n","    \n","   ![image-5.png](attachment:image-5.png)"]},{"cell_type":"code","execution_count":21,"metadata":{"id":"iMQOysMBl1zx","executionInfo":{"status":"ok","timestamp":1733158587319,"user_tz":-180,"elapsed":328,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["def sampler(pdf, train_test_ratio = [0.7, 0.15, 0.15], strategy='pick', number_test=3):\n","\n","    '''\n","    The function takes full datframe and undertakes sample partitioning\n","    pdf: is the dataframe that contains all hericane trajectories\n","    train_test_ratio: the ratio used for training, validation and testing respectively\n","    strategy: how sample is alloted, either using provided ratios or just user defined number\n","    number_test: if strategy is \"pick\" the number of test samples reserved for testing\n","    returns: list of unique hurricane IDs allocatied for trainning, vaidation and testing\n","    '''\n","\n","    inds = list(np.unique(pdf['ID']))  # take uniqe hericane IDS\n","\n","    random.seed(2)   # for reproduciability\n","    random.shuffle(inds) # shuffle\n","\n","    if strategy == 'pick':\n","        test_ids = random.sample(inds, number_test)\n","        train_ids = list(set(inds).symmetric_difference(set(test_ids)))\n","        return train_ids, test_ids\n","    else:\n","\n","        N = len(inds)\n","\n","        trn = round(N*train_test_ratio[0])\n","        vln = round(N*train_test_ratio[1])\n","        tsn = round(N*train_test_ratio[2])\n","\n","        train = inds[:trn]         # training hericanes\n","        valid = inds[trn:trn+vln]  # validation hericanes\n","        test = inds[trn+vln:]      # test hericanes\n","\n","        return train, valid, test"]},{"cell_type":"code","execution_count":22,"metadata":{"id":"AS2VGK3ml1zx","executionInfo":{"status":"ok","timestamp":1733158591693,"user_tz":-180,"elapsed":388,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["def prepare_samples(pdf, hid, remove_cols=['ID', 'Event', 'Status', 'Name'], w=3, normalize=False):\n","    '''\n","    Prepares samples by taking specific hurricane ID\n","    pdf: dataframe containing all attributes and labels\n","    hid: unique hurricane ID\n","    remove_cols: list of column/attribute names not included in the analysis\n","    W: window length to segment time sereis data\n","    normalize: boolean value whether toundertaken data normalization. If True, undrtakes minmax-normalization\n","    returns: prepared features with shape [B, T, A] and labels with shape [B, L], where L is number of sequence length to predict\n","    '''\n","\n","    samp = pdf[pdf['ID']==hid]  # subset specific hericane\n","\n","    if remove_cols is not None:\n","        feat_negin = samp.drop(remove_cols, axis=1)\n","    else:\n","        feat_negin = samp\n","#     feat = feat_negin.drop(['Latitude', 'Longitude'],  axis=1)\n","    feat = feat_negin\n","\n","    if normalize:\n","        min_max_scaler = preprocessing.MinMaxScaler()\n","        scaled = min_max_scaler.fit_transform(feat.values)\n","        col_names = list(feat.keys())\n","        feat = pd.DataFrame(scaled, columns=col_names)\n","\n","    label = feat_negin[['Latitude', 'Longitude']]\n","\n","    c = feat.shape[0]\n","    if c<=w:\n","        print('Hericane with ID: {} -> has length less than window and not included in the features'.format(hid))\n","        return None, None\n","    else:\n","        feature_map = []\n","        label_map = []\n","\n","        for i in range(c-w-1):\n","            FF = np.array(feat.iloc[i:i+w])\n","            LL = label.iloc[i+w+1]\n","            feature_map.append(FF)\n","            label_map.append(LL)\n","\n","        feature_map = np.array(feature_map)\n","        label_map = np.array(label_map)\n","\n","        # # for more caution\n","#         if len(feature_map.shape) == 2:\n","#             feature_map = np.expand_dims(feature_map, axis=0)\n","#         if len(label_map.shape) == 2:\n","#             label_map = np.expand_dims(label_map, axis=0)\n","\n","        return feature_map, label_map"]},{"cell_type":"code","execution_count":23,"metadata":{"id":"LHUY_RJ6l1zy","executionInfo":{"status":"ok","timestamp":1733158595821,"user_tz":-180,"elapsed":564,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["def prepare_features(IDS, pdfs, w=3):\n","    '''\n","    Undertakes batch processing for all hurricanes\n","    IDS: list of unique Hurricane IDs\n","    pdfs: data frame containing all features and labels columns\n","    w: window length to segment time series into datapoints suitable for LSTM input\n","    '''\n","\n","    features = []\n","    labels = []\n","\n","    for ids in IDS:\n","        data = prepare_samples(pdf=pdfs, hid=ids, w=w)\n","\n","        if data[0] is None or data[1] is None:\n","            pass\n","        else:\n","            if data[0].shape == 2 or data[1].shape == 2:\n","                data[0] = np.expand_dimes(data[0], axis=0)\n","                data[1] = np.expand_dimes(data[1], axis=0)\n","                features.append(data[0])\n","                labels.append(data[1])\n","            else:\n","                features.append(data[0])\n","                labels.append(data[1])\n","\n","    inds = [i for i in range(len(features)) if len(features[i].shape)==3]\n","    features = [features[j] for j in inds]\n","    labels = [labels[j] for j in inds]\n","\n","    features = np.concatenate(tuple(features), axis=0)\n","    labels = np.concatenate(tuple(labels), axis=0)\n","\n","    return features, labels"]},{"cell_type":"markdown","metadata":{"id":"ssWRgpkUl1zy"},"source":["## Select training and test hericanes before data preparation.\n","This is mainly bacause, the test data should be inspected independently using its unique hericane ID later on after trainning"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"AXOIvMX2l1zy","executionInfo":{"status":"ok","timestamp":1733161028213,"user_tz":-180,"elapsed":397,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["test_sample_intake = 'pick'\n","if test_sample_intake == 'pick':\n","    samples = sampler(PD_n)\n","    train_ids = samples[0]\n","    test_ids = samples[1]\n","    print('Length of training herican samples: {}'.format(len(train_ids)))\n","    print('Length of test herican samples: {}'.format(len(test_ids)))\n","else:\n","    samples = sampler(PD_n, strategy='ratio')\n","    train_ids = samples[0]\n","    validation_ids = samples[1]\n","    test_ids = samples[2]\n","    print('Length of training herican samples: {}'.format(len(train_ids)))\n","    print('Length of validation herican samples: {}'.format(len(validation_ids)))\n","    print('Length of test herican samples: {}'.format(len(test_ids)))"]},{"cell_type":"markdown","metadata":{"id":"XsRdo0hxl1zy"},"source":["###### Please note that as indicated below if there is herican event with observations less than or equal to proposed sequence length to model, it is excluded from being part of analysis\n","\n","# Prepare trainning features"]},{"cell_type":"code","execution_count":63,"metadata":{"id":"Vyd0s8Crl1zz","executionInfo":{"status":"ok","timestamp":1733161044182,"user_tz":-180,"elapsed":535,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["train_samples = prepare_features(IDS=train_ids, pdfs=PD_n, w=3)\n","train_feature = train_samples[0]\n","train_label = train_samples[1]\n","print('Shape of training features', train_feature.shape)\n","print('Shape of trainning labels',  train_label.shape)"]},{"cell_type":"markdown","metadata":{"id":"BOeMWdkXl1zz"},"source":["## Optional train test split\n","*Note: the following commented cells are used if we have sufficient amount of samples to partition*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ifP6ArKol1zz"},"outputs":[],"source":["# Prepare validation samples\n","# validation_samples = prepare_features(IDS=validation_ids, pdfs=PD_n)\n","# validation_feature = validation_samples[0]\n","# validation_label = validation_samples[1]\n","# validation_feature.shape, validation_label.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xSiUSekl1zz"},"outputs":[],"source":["# prepare test samples\n","# test_samples = prepare_features(IDS=test_ids, pdfs=PD_n)\n","# test_feature = validation_samples[0]\n","# test_label = validation_samples[1]\n","# test_feature.shape, test_label.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SFVAADOAl1z0"},"outputs":[],"source":["# combine, shuffle and sample again\n","# features = np.concatenate((train_feature,validation_feature, test_feature), axis=0)\n","# labels  = np.concatenate((train_label,validation_label, test_label), axis=0)\n","# features.shape, labels.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A9QZGMA1l1z0"},"outputs":[],"source":["# X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.05, random_state=42)\n","# X_train.shape, X_test.shape, y_train.shape, y_test.shape"]},{"cell_type":"markdown","metadata":{"id":"cGvdmXoOl1z0"},"source":["# Model design"]},{"cell_type":"code","execution_count":26,"metadata":{"id":"nG_X8PlMl1z1","executionInfo":{"status":"ok","timestamp":1733158648073,"user_tz":-180,"elapsed":383,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["# optimizers to use give a try and select one. In principle this should be done in a\n","# grid-search strategy but I have tried it with one by one basis\n","# sgd = SGD(learning_rate=0.001, momentum=0.9)\n","# adm = Adam(learning_rate=0.001)\n","msprop = RMSprop(learning_rate=0.0001,rho=0.9, momentum=0.9, centered=True)\n","# import tensorflow as tf\n"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"wD0Gr50Cl1z1","executionInfo":{"status":"ok","timestamp":1733161066827,"user_tz":-180,"elapsed":324,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["tf.keras.backend.clear_session()\n","data_shape = (train_feature.shape[1], train_feature.shape[-1])\n","model = Sequential()\n","model.add(Bidirectional(LSTM(500,return_sequences=True), input_shape=data_shape))\n","model.add(BatchNormalization())\n","model.add(Bidirectional(LSTM(500)))\n","model.add(BatchNormalization())\n","model.add(Dropout(0.2))\n","model.add(Dense(64))\n","model.add(Dropout(0.2))\n","model.add(Dense(2))\n","model.compile(loss='mse', optimizer=msprop)\n","model.summary()\n","checkpoint_path = '/content/drive/MyDrive/plus-deepdata-social-good-master/plus-deepdata-social-good-master/checkpoint/.weights.h5'\n","cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n","                                                 save_weights_only=True,\n","                                                 verbose=1)"]},{"cell_type":"markdown","source":["#Load pretrained weight"],"metadata":{"id":"lF9Kk5JIbdti"}},{"cell_type":"code","source":["try:\n","  model.load_weights(checkpoint_path)\n","  print(f'Pretrained weight loaded from: {checkpoint_path}')\n","except:\n","  print('pretrained weight is not loaded, starting training from scratch')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uB8ETVFXTpXn","executionInfo":{"status":"ok","timestamp":1733158791628,"user_tz":-180,"elapsed":3383,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}},"outputId":"80df3787-81d4-468f-9519-670a7df3e096"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Pretrained weight loaded from: /content/drive/MyDrive/plus-deepdata-social-good-master/plus-deepdata-social-good-master/checkpoint/.weights.h5\n"]}]},{"cell_type":"markdown","metadata":{"id":"FOTGZRWgl1z1"},"source":["# Train the model"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"R2iaGMi4l1z2","executionInfo":{"status":"ok","timestamp":1733161117605,"user_tz":-180,"elapsed":579,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["hist = model.fit(x=train_feature,\n","                 y=train_label,\n","                 epochs=15,\n","                 verbose=1,\n","                 callbacks=[cp_callback])"]},{"cell_type":"markdown","source":["# Test the performance of the model and visalize predictions"],"metadata":{"id":"p7kX121t3ein"}},{"cell_type":"code","execution_count":66,"metadata":{"id":"j_aoSGuCl1z2","executionInfo":{"status":"ok","timestamp":1733161136037,"user_tz":-180,"elapsed":358,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["# compute metrics and plot predicted and reference hericane trajectories\n","MSE = tf.keras.metrics.MeanSquaredError()   # mean squared error\n","MAE = tf.keras.metrics.MeanAbsoluteError()   # mean absolute error\n","\n","mse = []\n","mae = []\n","\n","for i in range(len(test_ids)):\n","    one_id = test_ids[i]\n","    tr_f = prepare_samples(pdf=PD_n, hid=one_id, w=3)  # prepare test data and ingest into trained model\n","    pred = hist.model.predict(tr_f[0])\n","\n","    MSE.update_state(tf.constant(tr_f[1]), pred)  # compute accuracy per test herican ID usimg predicted and refenrece data\n","    mse.append(MSE.result().numpy()) # append it\n","    MSE.reset_state()\n","\n","    MAE.update_state(tf.constant(tr_f[1]), pred)  # compute accuracy per test herican ID usimg predicted and refenrece data\n","    mae.append(MAE.result().numpy()) # append it\n","    MAE.reset_state()\n","\n","    plt.scatter(pred[:,1], pred[:,0])\n","    plt.plot(pred[:,1], pred[:,0], label='predicted')\n","    plt.scatter(tr_f[1][:,1], tr_f[1][:,0])\n","    plt.plot(tr_f[1][:,1], tr_f[1][:,0], label='observed')\n","    plt.xlabel('Longitude')\n","    plt.ylabel('Latitude')\n","    plt.title(one_id)\n","    plt.legend()\n","    plt.show()"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"EPe-a9F9l1z2","executionInfo":{"status":"ok","timestamp":1733161666930,"user_tz":-180,"elapsed":373,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["# check the mean squared error\n","print('Herican ID -> Mean Absolute Error  -> Mean Squred Error')\n","for i in range(len(test_ids)):\n","    print('{} -> {}  -> {}'.format(test_ids[i], mae[i], mse[i]))"]},{"cell_type":"markdown","source":["## Optional: Save the model weights:\n","If the model weights are not saved during the training phase the following cell could be unchecked and the model weights cold be save."],"metadata":{"id":"Zz3CBtW5Z7UC"}},{"cell_type":"code","execution_count":39,"metadata":{"id":"XWdl6Kj6l1z3","executionInfo":{"status":"ok","timestamp":1733160473008,"user_tz":-180,"elapsed":1948,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["# Save the trained model\n","# name = 'best_trajectory_model_weight.h5'\n","# if 'trained_model_weights' not in os.listdir(data_folder): # create model weight directory\n","#     model_path = os.path.join(data_folder, 'trained_model_weights')\n","#     os.makedirs(model_path, exist_ok=True)\n","#     model_name = os.path.join(model_path, name)\n","# else:\n","# model_path = os.path.join(data_folder, 'trained_model_weights')\n","# model_name = '/content/drive/MyDrive/plus-deepdata-social-good-master/trained_model_weights/weights.ckpt' # os.path.join(model_path, name)\n","# model.save_weights('.weights.h5') # save model weights"]},{"cell_type":"markdown","metadata":{"id":"dsEkKytal1z3"},"source":["# **Optional**\n","If need to load save weights first define the same model and load weights"]},{"cell_type":"code","execution_count":33,"metadata":{"id":"Xvf962Mil1z3","executionInfo":{"status":"ok","timestamp":1733160177802,"user_tz":-180,"elapsed":2746,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["# tf.keras.backend.clear_session()\n","# data_shape = (train_feature.shape[1], train_feature.shape[-1])\n","# new_model = Sequential()\n","# new_model.add(Bidirectional(LSTM(500,return_sequences=True), input_shape=data_shape))\n","# new_model.add(BatchNormalization())\n","# new_model.add(Bidirectional(LSTM(500)))\n","# new_model.add(BatchNormalization())\n","# new_model.add(Dropout(0.2))\n","# new_model.add(Dense(64)) # relu\n","# new_model.add(Dropout(0.2))   #\n","# new_model.add(Dense(2)) # out_size\n","# new_model.compile(loss='mse', optimizer=msprop)"]},{"cell_type":"markdown","source":["## Load model"],"metadata":{"id":"UHg5gFQua2E-"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"nNY16Pn5l1z4","executionInfo":{"status":"ok","timestamp":1733161443137,"user_tz":-180,"elapsed":1426,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["# checkpoint_path = '/content/drive/MyDrive/plus-deepdata-social-good-master/plus-deepdata-social-good-master/checkpoint/.weights.h5'\n","# if checkpoint_path is not None:\n","#     new_model.load_weights(checkpoint_path)\n","#     print('model weights loaded from {}'.format(checkpoint_path))"]},{"cell_type":"markdown","source":["## Perform test and visualize prediction reults"],"metadata":{"id":"TNrcF2Jya7IH"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"LDA91OuMl1z4","executionInfo":{"status":"ok","timestamp":1733161419100,"user_tz":-180,"elapsed":1395,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"outputs":[],"source":["# # compute metrics and plot predicted and reference hericane trajectories\n","# MSE = tf.keras.metrics.MeanSquaredError()   # mean squared error\n","# MAE = tf.keras.metrics.MeanAbsoluteError()   # mean absolute error\n","\n","# mse = []\n","# mae = []\n","\n","# for i in range(len(test_ids)):\n","#     one_id = test_ids[i]\n","#     tr_f = prepare_samples(pdf=PD_n, hid=one_id, w=3)  # prepare test data and ingest into trained model\n","#     pred = model.predict(tr_f[0])\n","\n","#     MSE.update_state(tf.constant(tr_f[1]), pred)  # compute accuracy per test herican ID usimg predicted and refenrece data\n","#     mse.append(MSE.result().numpy()) # append it\n","#     MSE.reset_state()\n","\n","#     MAE.update_state(tf.constant(tr_f[1]), pred)  # compute accuracy per test herican ID usimg predicted and refenrece data\n","#     mae.append(MAE.result().numpy()) # append it\n","#     MAE.reset_state()\n","\n","#     plt.scatter(pred[:,1], pred[:,0])\n","#     plt.plot(pred[:,1], pred[:,0], label='predicted')\n","#     plt.scatter(tr_f[1][:,1], tr_f[1][:,0])\n","#     plt.plot(tr_f[1][:,1], tr_f[1][:,0], label='observed')\n","#     plt.xlabel('Longitude')\n","#     plt.ylabel('Latitude')\n","#     plt.title(one_id)\n","#     plt.legend()\n","#     plt.show()"]},{"cell_type":"code","source":["# print('Herican ID -> Mean Absolute Error  -> Mean Squred Error')\n","# for i in range(len(test_ids)):\n","#     print('{} -> {}  -> {}'.format(test_ids[i], mae[i], mse[i]))"],"metadata":{"id":"AqMcEewaeIxM","executionInfo":{"status":"ok","timestamp":1733161431535,"user_tz":-180,"elapsed":337,"user":{"displayName":"Getachew Workineh","userId":"02924533880709860753"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"rwk-YM9ZNefC"},"execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}